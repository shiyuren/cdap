/*
 * Copyright Â© 2018 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package co.cask.cdap.datapipeline;

import co.cask.cdap.api.lineage.field.InputField;
import co.cask.cdap.api.lineage.field.Operation;
import co.cask.cdap.api.lineage.field.ReadOperation;
import co.cask.cdap.api.lineage.field.TransformOperation;
import co.cask.cdap.api.lineage.field.WriteOperation;
import co.cask.cdap.etl.api.lineage.field.PipelineOperation;
import co.cask.cdap.etl.api.lineage.field.PipelineReadOperation;
import co.cask.cdap.etl.api.lineage.field.PipelineTransformOperation;
import co.cask.cdap.etl.api.lineage.field.PipelineWriteOperation;
import co.cask.cdap.etl.planner.Dag;
import co.cask.cdap.etl.proto.Connection;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Joiner;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.ListIterator;
import java.util.Map;
import java.util.Set;
import java.util.TreeSet;

/**
 * Class responsible for processing the field lineage operations recorded by plugins and
 * converting it into the form the platform expects. This includes prefixing operation names
 * to make sure they are unique across all the plugins, adding implicit merge operations when
 * stage has more than one input etc.
 */
public class LineageOperationsProcessor {
  private static final String SEPARATOR = ".";
  private final List<String> topologicalOrder;
  private final Dag stageDag;
  // Map of stage to list of operations recorded by that stage
  private final Map<String, List<PipelineOperation>> stageOperations;
  // Map of stage name to another map which contains the output field names to the corresponding origin
  private final Map<String, Map<String, String>> stageOutputsWithOrigins;
  // Set of stages which requires no implicit merge operation for example stages of type join
  private final Set<String> noMergeRequiredStages;
  private Map<String, Operation> processedOperations;

  public LineageOperationsProcessor(Set<Connection> stageConnections, Map<String, Set<String>> stageInputFields,
                                    Map<String, Set<String>> stageOutputFields,
                                    Map<String, List<PipelineOperation>> stageOperations,
                                    Set<String> noMergeRequiredStages) {
    this.stageDag = new Dag(stageConnections);
    this.topologicalOrder = stageDag.getTopologicalOrder();
    this.stageOperations = stageOperations;
    this.stageOutputsWithOrigins = new HashMap<>();
    for (String stage : topologicalOrder) {
      stageOutputsWithOrigins.put(stage, new LinkedHashMap<>());
    }
    this.noMergeRequiredStages = noMergeRequiredStages;
    validate(stageInputFields, stageOutputFields, stageOperations);
  }

  /**
   * Validate the inputs and outputs of operations.
   *
   * For each operation the input must be coming from the input schema that stage receives
   * or it must be one of the output of operations recorded by that stage prior to the
   * current operation. For example, consider that stage gets input fields as [a, b, c].
   * First operation recorded by the stage generates outputs [d, e]. For any subsequent
   * operation, the valid input set will be [a, b, c, d, e].
   *
   * For each operation the output generated by it must be the part of output schema or
   * used as an input by the subsequent operations recorded by the stage. For example,
   * if operation OP1 creates outputs [a, b, c, x] and stage has output fields as [a, b, d, e],
   * then output c must be consumed by any subsequent operation, otherwise validation
   * fails. It is also possible to generate the redundant outputs by operations. For example,
   * consider other operations OP2 with inputs as [c] and output as [x], and OP3 with input as
   * [X] and output as [b]. In this case the output field [b] created by OP1 is redundant,
   * since the field b of the output schema will come from OP3 and b is not used as input by any
   * operation subsequent of OP1. Validation fails in this case.
   *
   * @param stageInputFields stage and corresponding set of input fields
   * @param stageOutputFields stage and corresponding set of output fields
   * @param stageOperations stage and corresponding List of field operations
   * @throws IllegalArgumentException when the validation fails
   */
  @VisibleForTesting
  void validate(Map<String, Set<String>> stageInputFields, Map<String, Set<String>> stageOutputFields,
                Map<String, List<PipelineOperation>> stageOperations) {
    // For a given stage, following map stores the name of the field to the List of operations
    // for which the field appears in the output. Eventually this map will only contains the fields
    // which are not used by any subsequent operation and are also not part of output schema of that stage.
    Map<String, Map<String, List<String>>> unusedOutputsFromStage = new HashMap<>();

    // For a given stage, following map stores the name of the field to the List of operations
    // for which the field appears in the input. Eventually this map will only contains the fields
    // which are not part of the input schema of that stage and also not outputted by any of the operations.
    Map<String, Map<String, List<String>>> invalidOperationInputs = new HashMap<>();

    for (Map.Entry<String, List<PipelineOperation>> stageOperation : stageOperations.entrySet()) {
      String stageName = stageOperation.getKey();
      List<PipelineOperation> operations = stageOperation.getValue();

      // map of unused outputs fields for the current stage to the list of operations that outputted
      // those fields
      Map<String, List<String>> unusedOperationOutputs = new HashMap<>();

      // map of output fields which are redundant (overwritten by subsequent operations
      // without using them) to the list of operations which create such redundancy
      Map<String, List<String>> redundantOutputs = new HashMap<>();

      Set<String> validOperationInputs = new HashSet<>(stageInputFields.get(stageName));

      // map of field to the operations which are using that field which are not part of
      // input schema and also not generated by any operations
      Map<String, List<String>> invalidInputs = new HashMap<>();

      for (PipelineOperation pipelineOperation : operations) {
        switch (pipelineOperation.getType()) {
          case READ:
            PipelineReadOperation read = (PipelineReadOperation) pipelineOperation;
            processInputOutputs(read.getName(), Collections.emptyList(), read.getOutputFields(), validOperationInputs,
                                invalidInputs, unusedOperationOutputs, redundantOutputs);
            break;
          case TRANSFORM:
            PipelineTransformOperation transform = (PipelineTransformOperation) pipelineOperation;
            processInputOutputs(transform.getName(), transform.getInputFields(), transform.getOutputFields(),
                                validOperationInputs, invalidInputs, unusedOperationOutputs, redundantOutputs);
            break;
          case WRITE:
            PipelineWriteOperation write = (PipelineWriteOperation) pipelineOperation;
            processInputOutputs(write.getName(), write.getInputFields(), Collections.emptyList(), validOperationInputs,
                                invalidInputs, unusedOperationOutputs, redundantOutputs);
            break;
        }
      }

      // Remove outputs from unusedOperationOutputs map which are not used by any operation as an input,
      // however they are part of output schema. We cannot simply remove all keys here by doing
      // "unusedOperationOutputs.removeAll(stageOutputFields.get(stageName))". Reason for this is
      // consider the case where OP1 outputs field b, and OP2 outputs the same field b. Operation OP1 is
      // redundant, since b will be overwritten by OP2. However since field b is  not consumed by any other
      // operation as an input, redundantOutputs will not have its entry. We still want to fail the validation
      // for operation OP1.
      Iterator<Map.Entry<String, List<String>>> iterator = unusedOperationOutputs.entrySet().iterator();
      while (iterator.hasNext()) {
        Map.Entry<String, List<String>> next = iterator.next();
        String field = next.getKey();
        List<String> origins = next.getValue();

        if (stageOutputFields.get(stageName).contains(field)) {
          iterator.remove();
          if (origins.size() > 1) {
            unusedOperationOutputs.put(field, origins.subList(0, origins.size() - 1));
          }
        }
      }

      unusedOperationOutputs.putAll(redundantOutputs);

      if (!unusedOperationOutputs.isEmpty()) {
        unusedOutputsFromStage.put(stageName, unusedOperationOutputs);
      }

      if (!invalidInputs.isEmpty()) {
        invalidOperationInputs.put(stageName, invalidInputs);
      }
    }

    if (unusedOutputsFromStage.isEmpty() && invalidOperationInputs.isEmpty()) {
      return;
    }

    String message = invalidOutputErrorMessage(unusedOutputsFromStage);
    message += invalidInputErrorMessage(invalidOperationInputs);
    throw new IllegalArgumentException(message);
  }

  private void processInputOutputs(String operationName, List<String> inputs, List<String> outputs,
                                   Set<String> validOperationInputs, Map<String, List<String>> invalidInputs,
                                   Map<String, List<String>> unusedOperationOutputs,
                                   Map<String, List<String>> redundantOutputs) {
    for (String field : inputs) {
      // check if field is valid input. input is valid if it is in the validOperationInputs set
      if (!validOperationInputs.contains(field)) {
        List<String> originsWithInvalidInput = invalidInputs.computeIfAbsent(field, k -> new ArrayList<>());
        originsWithInvalidInput.add(operationName);
      }

      // current input field can be removed from unusedOperationOutputs set now
      // as it is being used by current operation
      List<String> origins = unusedOperationOutputs.get(field);
      // origins can be null if the field is coming directly from the input schema
      if (origins != null) {
        if (origins.size() > 1) {
          // field is outputted by multiple operations. all occurrences of outputs are redundant
          // except the last one
          List<String> redundantOrigins = redundantOutputs.computeIfAbsent(field, k -> new ArrayList<>());
          redundantOrigins.addAll(origins.subList(0, origins.size() - 1));
        }
        // remove the field so that any occurrence of it later on can be processed as a new field.
        unusedOperationOutputs.remove(field);
      }
    }

    // put all outputs of the current operation in unusedOperationOutputs
    for (String field : outputs) {
      List<String> origins = unusedOperationOutputs.computeIfAbsent(field, k -> new ArrayList<>());
      origins.add(operationName);
    }
    // put all outputs in the valid input set
    validOperationInputs.addAll(outputs);
  }

  private String invalidOutputErrorMessage(Map<String, Map<String, List<String>>> unusedOutputsFromStage) {
    if (unusedOutputsFromStage.isEmpty()) {
      return "";
    }

    return String.format("Outputs of following operations are neither used by subsequent operations " +
                                     "in that stage nor are part of the output schema of that stage: %s. ",
                         invalidErrorMessage(unusedOutputsFromStage));
  }

  private String invalidInputErrorMessage(Map<String, Map<String, List<String>>> invalidOperationInputs) {
    if (invalidOperationInputs.isEmpty()) {
      return "";
    }

    return String.format("Inputs of following operations are neither part of the input schema of a stage nor are " +
                           "generated by any previous operations recorded by that stage: %s. ",
                         invalidErrorMessage(invalidOperationInputs));
  }

  private String invalidErrorMessage(Map<String, Map<String, List<String>>> stageFieldOperations) {
    StringBuilder stageFieldOperationBuilder = new StringBuilder();
    for (Map.Entry<String, Map<String, List<String>>> unusedOutputs : stageFieldOperations.entrySet()) {
      if (stageFieldOperationBuilder.length() != 0) {
        stageFieldOperationBuilder.append(", ");
      }

      stageFieldOperationBuilder.append("<stage:");
      stageFieldOperationBuilder.append(unusedOutputs.getKey());
      stageFieldOperationBuilder.append(", ");
      StringBuilder fieldOperationBuilder = new StringBuilder();
      for (Map.Entry<String, List<String>> fieldOperations : unusedOutputs.getValue().entrySet()) {
        for (String operation : fieldOperations.getValue()) {
          if (fieldOperationBuilder.length() != 0) {
            fieldOperationBuilder.append(", ");
          }
          fieldOperationBuilder.append("[operation:");
          fieldOperationBuilder.append(operation);
          fieldOperationBuilder.append(", field:");
          fieldOperationBuilder.append(fieldOperations.getKey());
          fieldOperationBuilder.append("]");
        }
      }
      stageFieldOperationBuilder.append(fieldOperationBuilder);
      stageFieldOperationBuilder.append(">");
    }
    return stageFieldOperationBuilder.toString();
  }

  /**
   * @return the operations which will be submitted to the platform
   */
  public Set<Operation> process() {
    if (processedOperations == null) {
      processedOperations = computeProcessedOperations();
    }
    return new HashSet<>(processedOperations.values());
  }

  private Map<String, Operation> computeProcessedOperations() {
    Map<String, Operation> processedOperations = new HashMap<>();
    for (String stageName : topologicalOrder) {
      Set<String> stageInputs = stageDag.getNodeInputs(stageName);
      if (stageInputs.size() > 1 && !noMergeRequiredStages.contains(stageName)) {
        addMergeOperation(stageInputs, processedOperations);
      }
      List<PipelineOperation> pipelineOperations = stageOperations.get(stageName);
      for (PipelineOperation pipelineOperation : pipelineOperations) {
        Operation newOperation = null;
        String newOperationName =  prefixedOperationName(stageName, pipelineOperation.getName());
        Set<String> currentOperationOutputs = new LinkedHashSet<>();
        switch (pipelineOperation.getType()) {
          case READ:
            PipelineReadOperation read = (PipelineReadOperation) pipelineOperation;
            newOperation = new ReadOperation(newOperationName, read.getDescription(),
                                              read.getSource(), read.getOutputFields());
            currentOperationOutputs.addAll(read.getOutputFields());
            break;
          case TRANSFORM:
            PipelineTransformOperation transform = (PipelineTransformOperation) pipelineOperation;
            List<InputField> inputFields = createInputFields(transform.getInputFields(), stageName,
                                                             processedOperations);
            newOperation = new TransformOperation(newOperationName, transform.getDescription(), inputFields,
                                                  transform.getOutputFields());
            currentOperationOutputs.addAll(transform.getOutputFields());
            break;
          case WRITE:
            PipelineWriteOperation write = (PipelineWriteOperation) pipelineOperation;
            inputFields = createInputFields(write.getInputFields(), stageName, processedOperations);
            newOperation = new WriteOperation(newOperationName, write.getDescription(), write.getSink(), inputFields);
            break;
        }
        for (String currentOperationOutput : currentOperationOutputs) {
          // For all fields outputted by the current operation assign the operation name as origin
          // If the field appears in the output again for some other operation belonging to the same stage,
          // its origin will get updated to the new operation
          stageOutputsWithOrigins.get(stageName).put(currentOperationOutput, newOperation.getName());
        }
        processedOperations.put(newOperation.getName(), newOperation);
      }
    }
    return processedOperations;
  }

  private void addMergeOperation(Set<String> stageInputs,
                                 Map<String, Operation> processedOperations) {
    Set<String> sortedInputs = new TreeSet<>(stageInputs);
    String mergeOperationName = prefixedOperationName(Joiner.on(SEPARATOR).join(sortedInputs), "merge");
    String mergeDescription = "Merging stages: " + Joiner.on(",").join(sortedInputs);
    if (processedOperations.containsKey(mergeOperationName)) {
      // it is possible that same stages act as an input to multiple stages.
      // we should still only add single merge operation for them
      return;
    }
    List<InputField> inputFields = new ArrayList<>();
    for (String inputStage : sortedInputs) {
      List<String> parentStages = findParentStages(inputStage);
      for (String parentStage : parentStages) {
        Map<String, String> fieldOrigins = stageOutputsWithOrigins.get(parentStage);
        for (Map.Entry<String, String> fieldOrigin : fieldOrigins.entrySet()) {
          inputFields.add(InputField.of(fieldOrigin.getValue(), fieldOrigin.getKey()));
        }
      }
    }

    Set<String> outputs = new LinkedHashSet<>();
    for (InputField inputField : inputFields) {
      outputs.add(inputField.getName());
    }
    TransformOperation merge = new TransformOperation(mergeOperationName, mergeDescription, inputFields,
                                                      new ArrayList<>(outputs));
    processedOperations.put(merge.getName(), merge);
  }

  /**
   * Create {@link InputField}s from field names which acts as an input to the operation. Creating
   * InputField requires origin; the name of the operation which created the field. To figure out the
   * origin, we traverse in the reverse direction from the current stage until we reach source. While
   * traversing, for each stage we check if the field occurs in any of the output operations recorded
   * by that stage, if so we have found the origin.
   *
   * @param fields the List of input field names for an operation for which InputFields to be created
   * @param currentStage name of the stage which recorded the operation
   * @param processedOperations processed operations so far
   * @return List of InputFields
   */
  private List<InputField> createInputFields(List<String> fields, String currentStage,
                                             Map<String, Operation> processedOperations) {
    // We need to return InputFields in the same order as fields.
    // Keep them in map so we can iterate later on received fields to return InputFields.
    Map<String, InputField> inputFields = new HashMap<>();
    List<String> parents = findParentStages(currentStage);
    for (String field : fields) {
      ListIterator<String> parentsIterator = parents.listIterator(parents.size());
      while (parentsIterator.hasPrevious()) {
        // Iterate in parents in the reverse order so that we find the most recently updated origin
        // first
        String stage = parentsIterator.previous();
        // check if the current field is output by any one of operation created by current stage
        String origin = stageOutputsWithOrigins.get(stage).get(field);
        if (origin != null) {
          inputFields.put(field, InputField.of(origin, field));
          break;
        }
      }
    }

    Set<String> stageInputs = stageDag.getNodeInputs(parents.get(0));
    if (stageInputs.size() > 1 && !noMergeRequiredStages.contains(currentStage)) {
      String mergeOperationName = mergeOperationName(stageInputs);
      Operation operation = processedOperations.get(mergeOperationName);
      List<String> outputs = ((TransformOperation) operation).getOutputs();
      for (String field : fields) {
        // Only add the InputFields corresponding to the remaining fields
        if (outputs.contains(field) && inputFields.get(field) == null) {
          inputFields.put(field, InputField.of(mergeOperationName, field));
        }
      }
    }

    List<InputField> result = new ArrayList<>();
    for (String field : fields) {
      if (inputFields.containsKey(field)) {
        result.add(inputFields.get(field));
      }
    }
    return result;
  }

  private List<String> findParentStages(String stageName) {
    List<String> parents = new ArrayList<>();
    String currentStage = stageName;
    while (true) {
      List<String> currentBranch = stageDag.getBranch(currentStage, new HashSet<>());
      parents.addAll(0, currentBranch);
      String headStage = parents.get(0);
      Set<String> nodeInputs = stageDag.getNodeInputs(headStage);
      if (nodeInputs.size() == 1) {
        // It is possible that the input node to the currentBranch has multiple outputs
        // Include the nodes on that branch as well.
        // For example:
        //               |--->n2--->n3
        //     n0-->n1-->|
        //               |--->n4--->n5
        //
        // If we want to find all parent stages of n3, stageDag.getBranch("n3") will only give us the branch
        // n2--->n3. However we still need to include n0-->n1 branch in the result for finding origins
        // since n1 has two outputs we know that its a different branch and need to be added
        String branchInputNode = nodeInputs.iterator().next();
        if (stageDag.getNodeOutputs(branchInputNode).size() > 1) {
          currentStage = branchInputNode;
          continue;
        }
      }
      break;
    }
    return parents;
  }

  private String mergeOperationName(Set<String> inputStages) {
    Set<String> sortedInputs = new TreeSet<>(inputStages);
    return prefixedOperationName(Joiner.on(SEPARATOR).join(sortedInputs), "merge");
  }

  private String prefixedOperationName(String stageName, String operationName) {
    return stageName + SEPARATOR + operationName;
  }
}
